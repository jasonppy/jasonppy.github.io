---
redirect_from: /about/
permalink: /
---

Hi! I'm Puyuan Peng, a third year Computer Science PhD student at [UT Austin](https://www.utexas.edu/), working mainly on speech processing and multimodal learning. I'm very fortunate to have [David Harwath](https://www.cs.utexas.edu/~harwath/) as my advisor.

In addition to my advisor, I have the pleasure to work with and learn from many amazing senior researchers, including (in chronological order): [Karen Livescu](https://home.ttic.edu/~klivescu/), [Herman Kamper](https://www.kamperh.com/), [Raymond Mooney](https://www.cs.utexas.edu/~mooney/), [James Glass](https://people.csail.mit.edu/jrg/), [Abdelrahman Mohamed](https://scholar.google.com/citations?hl=en&user=tJ_PrzgAAAAJ), [Jonathan Le Roux](https://www.jonathanleroux.org/), [Chiori Hori](https://www.merl.com/people/chori), [Shinji Watanabe](https://sites.google.com/view/shinjiwatanabe), [Shang-Wen Li](https://swdanielli.github.io/), [Okko Räsänen](https://webpages.tuni.fi/specog/index.html), [Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/index.php) etc.

I have a Master's degree in Statistics from [The University of Chicago](https://stat.uchicago.edu/alumni/ms-alumni/), and a Bachelor's degree in Mathematics from [Beijing Normal University](https://english.bnu.edu.cn/). 

In my free time, I try to workout and sing ([here](https://youtu.be/h-7TFc5pBuk) is a funny video).  

contact: pyp [at] utexas [dot] edu  

## Papers 
(The asterisk '\*' denotes equal contribution)  

<ol reversed>
  <li>
    <strong>AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models</strong><br>
    Yuan Tseng, Layne Berry*, Yi-Ting Chen*, I-Hsiang Chiu*, Hsuan-Hao Lin*, Max Liu*, <span style="font-weight: 550;">Puyuan Peng*</span>, Yi-Jen Shih*, Hung-Yu Wang*, Haibin Wu*, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee<br>
    <em>Preprint, 2023</em><br>
    <a href="https://arxiv.org/pdf/2309.10787.pdf">pdf</a> <a href="https://github.com/roger-tseng/av-superb">code</a> <a href="https://av.superbbenchmark.org/">website</a>
  </li>
  
  <li>
    <strong>Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Brian Yan, Shinji Watanabe, David Hawarth<br>
    <em>Interspeech, 2023</em><br>
    <a href="https://arxiv.org/pdf/2305.11095.pdf">pdf</a> <a href="https://github.com/jasonppy/promptingwhisper">code</a>
  </li>

  <li>
    <strong>Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Shang-Wen Li, Okko Räsänen, Abdelrahman Mohamed, David Harwath<br>
    <em>Interspeech, 2023</em><br>
    <a href="https://arxiv.org/pdf/2305.11435.pdf">pdf</a> <a href="https://github.com/jasonppy/syllable-discovery">code</a>
  </li>

  <li>
    <strong>Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos</strong><br>
    Chiori Hori, <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath, Xinyu Liu, Kei Ota, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux<br>
    <em>Interspeech, 2023</em><br>
    <a href="https://arxiv.org/pdf/2306.15644.pdf">pdf</a>
  </li>

  <li>
    <strong>Audio-Visual Neural Syntax Acquisition</strong><br>
    Cheng-I Jeff Lai*, Freda Shi*, <span style="font-weight: 550;">Puyuan Peng*</span>, Yoon Kim, Kevin Gimpel, Shiyu Chang, Yung-Sung Chuang, Saurabhchand Bhati, David Cox, David Harwath, Yang Zhang, Karen Livescu, James Glass<br>
    <em>ASRU, 2023</em><br>
    <a href="https://arxiv.org/pdf/2310.07654.pdf">pdf</a> <a href="https://github.com/jefflai108/AV-NSL">code</a>
  </li>

  <li>
    <strong>Zero-shot Video Moment Retrieval With Off-the-Shelf Models</strong><br>
    Anuj Diwan*, <span style="font-weight: 550;">Puyuan Peng*</span>, Raymond J. Mooney<br>
    <em>Workshop on Transfer Learning for Natural Language Processing, 2022</em><br>
    <a href="https://arxiv.org/pdf/2211.02178.pdf">pdf</a>
  </li>

  <li>
    <strong>Word Discovery in Visually Grounded, Self-Supervised Speech Models</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>Interspeech, 2022</em><br>
    <a href="https://arxiv.org/pdf/2203.15081.pdf">pdf</a> <a href="https://github.com/jasonppy/word-discovery">code</a>
  </li>

  <li>
    <strong>MAE-AST: Masked Autoencoding Audio Spectrogram Transformer</strong><br>
    Alan Baade, <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>Interspeech, 2022</em><br>
    <a href="https://arxiv.org/pdf/2203.16691.pdf">pdf</a> <a href="https://github.com/AlanBaade/MAE-AST-Public">code</a>
  </li>

  <li>
    <strong>Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>The 2nd Workshop on Self-supervised Learning for Audio and Speech Processing at AAAI, 2022</em><br>
    <a href="https://arxiv.org/pdf/2202.03543.pdf">pdf</a> <a href="https://github.com/jasonppy/FaST-VGS-Family">code</a>
  </li>

  <li>
    <strong>Fast-Slow Transformer for Visually Grounding Speech</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>ICASSP, 2022</em><br>
    <a href="https://arxiv.org/pdf/2109.08186.pdf">pdf</a> <a href="https://github.com/jasonppy/FaST-VGS-Family">code</a>
  </li>

  <li>
    <strong>A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Herman Kamper, and Karen Livescu<br>
    <em>The 1st Workshop on Self-Supervised Learning for Speech and Audio Processing at NeurIPS, 2020</em><br>
    <a href="https://arxiv.org/pdf/2012.02221.pdf">pdf</a>
  </li>
</ol>

<!-- **AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models**  
Yuan Tseng, Layne Berry\*, Yi-Ting Chen\*, I-Hsiang Chiu\*, Hsuan-Hao Lin\*, Max Liu\*, <u>Puyuan Peng*</u>, Yi-Jen Shih\*, Hung-Yu Wang\*, Haibin Wu\*, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee  
*Preprint, 2023*  
[pdf](https://arxiv.org/pdf/2309.10787.pdf) [code](https://github.com/roger-tseng/av-superb) [website](https://av.superbbenchmark.org/)  


**Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization**  
<u>Puyuan Peng</u>, Brian Yan, Shinji Watanabe, David Hawarth  
*Interspeech, 2023*  
[pdf](https://arxiv.org/abs/2305.11095) [code](https://github.com/jasonppy/promptingwhisper)

**Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model**  
<u>Puyuan Peng</u>, Shang-Wen Li, Okko Räsänen, Abdelrahman Mohamed, David Harwath  
*Interspeech, 2023*  
[pdf](https://arxiv.org/abs/2305.11435) [code](https://github.com/jasonppy/syllable-discovery)  

**Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos**  
Chiori Hori, <u>Puyuan Peng</u>, David Harwath, Xinyu Liu, Kei Ota, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux  
*Interspeech, 2023*  
[pdf](https://arxiv.org/abs/2306.15644)  

**Audio-Visual Neural Syntax Acquisition**  
Cheng-I Jeff Lai\*, Freda Shi\*, <u>Puyuan Peng*</u>, Yoon Kim, Kevin Gimpel, Shiyu Chang, Yung-Sung Chuang, Saurabhchand Bhati, David Cox, David Harwath, Yang Zhang, Karen Livescu, James Glass  
*ASRU, 2023*  

**Zero-shot Video Moment Retrieval With Off-the-Shelf Models**  
Anuj Diwan\*, <u>Puyuan Peng*</u>, Raymond J. Mooney  
*Workshop on Transfer Learning for Natural Language Processing, 2022*   
[pdf](https://arxiv.org/pdf/2211.02178.pdf)

**Word Discovery in Visually Grounded, Self-Supervised Speech Models**  
<u>Puyuan Peng</u>, David Harwath  
*Interspeech, 2022*  
[pdf](https://arxiv.org/pdf/2203.15081.pdf) [code](https://github.com/jasonppy/word-discovery)  

**MAE-AST: Masked Autoencoding Audio Spectrogram Transformer**  
Alan Baade, <u>Puyuan Peng</u>, David Harwath  
*Interspeech, 2022*   
[pdf](https://arxiv.org/pdf/2203.16691.pdf) [code](https://github.com/AlanBaade/MAE-AST-Public)

**Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling**  
<u>Puyuan Peng</u>, David Harwath  
*The 2nd Workshop on Self-supervised Learning for Audio and Speech Processing at AAAI, 2022*  
[pdf](https://arxiv.org/pdf/2202.03543.pdf) [code](https://github.com/jasonppy/FaST-VGS-Family)  

**Fast-Slow Transformer for Visually Grounding Speech**  
<u>Puyuan Peng</u>, David Harwath  
*ICASSP, 2022*  
[pdf](https://arxiv.org/pdf/2109.08186.pdf) [code](https://github.com/jasonppy/FaST-VGS-Family)

**A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings**  
<u>Puyuan Peng</u>, Herman Kamper, and Karen Livescu  
*The 1st Workshop on Self-Supervised Learning for Speech and Audio Processing at NeurIPS, 2020*  
[pdf](https://arxiv.org/abs/2012.02221) -->

## Talks
May 2022 at [Developmental Intelligence Laboratory](https://www.la.utexas.edu/users/dil/), Department of Psychology, UT Austin, USA  
Jan 2022 at [Karen Livescu Group](https://home.ttic.edu/~klivescu/),  Toyota Technological Institute at Chicago, USA.  
Jan 2022 at [Cognitive Machine Learning Group](https://cognitive-ml.fr/), Departement d’Etudes Cognitives, Ecole Normale Supérieure, France.  