---
redirect_from: /about/
permalink: /
---

Hi! I'm Puyuan Peng, a third year Computer Science PhD student at [UT Austin](https://www.utexas.edu/). I mainly work on speech/audio recognition, understanding, and generation, usually under multimodal context (e.g. text, vision). **I'll be graduating in the Spring of 2025 and on the job market, please reach out if interested!**

I'm very fortunate to have [David Harwath](https://www.cs.utexas.edu/~harwath/) as my advisor. In addition to my advisor, I have the pleasure to work with and learn from many amazing senior researchers, including (in chronological order): [Karen Livescu (TTIC/UChicago)](https://home.ttic.edu/~klivescu/), [Raymond Mooney (UT)](https://www.cs.utexas.edu/~mooney/), [James Glass (MIT)](https://people.csail.mit.edu/jrg/), [Yoon Kim (MIT)](https://people.csail.mit.edu/yoonkim/), [Abdelrahman Mohamed (Rembrand)](https://scholar.google.com/citations?hl=en&user=tJ_PrzgAAAAJ), [Jonathan Le Roux (MERL)](https://www.jonathanleroux.org/), [Shinji Watanabe (CMU)](https://sites.google.com/view/shinjiwatanabe), [Hung-yi Lee (NTU)](https://speech.ee.ntu.edu.tw/~hylee/index.php), [Kristen Grauman (UT/Meta)](https://www.cs.utexas.edu/users/grauman/), [Wei-Ning Hsu (Meta)](https://scholar.google.com/citations?user=N5HDmqoAAAAJ&hl=en) etc.

I have a Master's degree in Statistics from [The University of Chicago](https://stat.uchicago.edu/alumni/ms-alumni/), and a Bachelor's degree in Mathematics from [Beijing Normal University](https://english.bnu.edu.cn/).  

Start from the Summer of 2024, I'm interning at FAIR at [Meta](https://ai.meta.com/), working with [Wei-Ning Hsu](https://scholar.google.com/citations?user=N5HDmqoAAAAJ).

In my free time, I like to workout and sing. 

contact: pyp@utexas.edu  

## Papers 
(The asterisk '\*' denotes equal contribution)  

<ol reversed>
  <li>
    <strong>VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Po-Yao Huang, Abdelrahman Mohamed, David Harwath<br>
    <em>ACL, 2024</em><br>
    <a href="/assets/pdfs/VoiceCraft.pdf">pdf</a> <a href="https://jasonppy.github.io/VoiceCraft_web/">website</a> <a href="https://huggingface.co/spaces/pyp1/VoiceCraft_gradio">interactive demo</a> <a href="https://github.com/jasonppy/VoiceCraft">code</a> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jasonppy/VoiceCraft">
  </li>
  <li>
    <strong>BAT: Learning to Reason about Spatial Sounds with Large Language Models</strong><br>
    Zhisheng Zheng, <span style="font-weight: 550;">Puyuan Peng</span>, Ziyang Ma, Xie Chen, Eunsol Choi, David Harwath<br>
    <em>ICML, 2024</em><br>
    <a href="https://arxiv.org/pdf/2402.01591.pdf">pdf</a> <a href="https://github.com/zszheng147/Spatial-AST">Spatial-AST code</a> <a href="https://github.com/zszheng147/Spatial-AST">BAT code (comming soon)</a> 
  </li>
  <li>
    <strong>AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models</strong><br>
    Yuan Tseng, Layne Berry*, Yi-Ting Chen*, I-Hsiang Chiu*, Hsuan-Hao Lin*, Max Liu*, <span style="font-weight: 550;">Puyuan Peng*</span>, Yi-Jen Shih*, Hung-Yu Wang*, Haibin Wu*, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee<br>
    <em>ICASSP, 2024</em><br>
    <a href="https://arxiv.org/pdf/2309.10787.pdf">pdf</a> <a href="https://github.com/roger-tseng/av-superb">code</a> <a href="https://av.superbbenchmark.org/">website</a>
  </li>
  
  <li>
    <strong>Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Brian Yan, Shinji Watanabe, David Hawarth<br>
    <em>Interspeech, 2023</em><br>
    <a href="https://arxiv.org/pdf/2305.11095.pdf">pdf</a> <a href="https://github.com/jasonppy/promptingwhisper">code</a>
  </li>

  <li>
    <strong>Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Shang-Wen Li, Okko Räsänen, Abdelrahman Mohamed, David Harwath<br>
    <em>Interspeech, 2023</em><br>
    <a href="https://arxiv.org/pdf/2305.11435.pdf">pdf</a> <a href="https://github.com/jasonppy/syllable-discovery">code</a>
  </li>

  <li>
    <strong>Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos</strong><br>
    Chiori Hori, <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath, Xinyu Liu, Kei Ota, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux<br>
    <em>Interspeech, 2023</em><br>
    <a href="https://arxiv.org/pdf/2306.15644.pdf">pdf</a>
  </li>

  <li>
    <strong>Audio-Visual Neural Syntax Acquisition</strong><br>
    Cheng-I Jeff Lai*, Freda Shi*, <span style="font-weight: 550;">Puyuan Peng*</span>, Yoon Kim, Kevin Gimpel, Shiyu Chang, Yung-Sung Chuang, Saurabhchand Bhati, David Cox, David Harwath, Yang Zhang, Karen Livescu, James Glass<br>
    <em>ASRU, 2023</em><br>
    <a href="https://arxiv.org/pdf/2310.07654.pdf">pdf</a> <a href="https://github.com/jefflai108/AV-NSL">code</a>
  </li>

  <li>
    <strong>Zero-shot Video Moment Retrieval With Off-the-Shelf Models</strong><br>
    Anuj Diwan*, <span style="font-weight: 550;">Puyuan Peng*</span>, Raymond J. Mooney<br>
    <em>Workshop on Transfer Learning for Natural Language Processing, 2022</em><br>
    <a href="https://arxiv.org/pdf/2211.02178.pdf">pdf</a>
  </li>

  <li>
    <strong>Word Discovery in Visually Grounded, Self-Supervised Speech Models</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>Interspeech, 2022</em><br>
    <a href="https://arxiv.org/pdf/2203.15081.pdf">pdf</a> <a href="https://github.com/jasonppy/word-discovery">code</a>
  </li>

  <li>
    <strong>MAE-AST: Masked Autoencoding Audio Spectrogram Transformer</strong><br>
    Alan Baade, <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>Interspeech, 2022</em><br>
    <a href="https://arxiv.org/pdf/2203.16691.pdf">pdf</a> <a href="https://github.com/AlanBaade/MAE-AST-Public">code</a>
  </li>

  <li>
    <strong>Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>The 2nd Workshop on Self-supervised Learning for Audio and Speech Processing at AAAI, 2022</em><br>
    <a href="https://arxiv.org/pdf/2202.03543.pdf">pdf</a> <a href="https://github.com/jasonppy/FaST-VGS-Family">code</a>
  </li>

  <li>
    <strong>Fast-Slow Transformer for Visually Grounding Speech</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, David Harwath<br>
    <em>ICASSP, 2022</em><br>
    <a href="https://arxiv.org/pdf/2109.08186.pdf">pdf</a> <a href="https://github.com/jasonppy/FaST-VGS-Family">code</a>
  </li>

  <li>
    <strong>A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings</strong><br>
    <span style="font-weight: 550;">Puyuan Peng</span>, Herman Kamper, and Karen Livescu<br>
    <em>The 1st Workshop on Self-Supervised Learning for Speech and Audio Processing at NeurIPS, 2020</em><br>
    <a href="https://arxiv.org/pdf/2012.02221.pdf">pdf</a>
  </li>
</ol>

## Talks
May 2022 at [Developmental Intelligence Laboratory](https://www.la.utexas.edu/users/dil/), Department of Psychology, UT Austin, USA  
Jan 2022 at [Karen Livescu Group](https://home.ttic.edu/~klivescu/),  Toyota Technological Institute at Chicago, USA.  
Jan 2022 at [Cognitive Machine Learning Group](https://cognitive-ml.fr/), Departement d’Etudes Cognitives, Ecole Normale Supérieure, France.  
